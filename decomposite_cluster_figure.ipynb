!pip install sklearn
!pip install plotly
!pip install MulticoreTSNE
!pip install umap-learn

import sys, datetime, pathlib, re
import plotly
import plotly.graph_objs as go
import plotly.offline
import sklearn.cluster
import pandas as pd
import numpy as np
import sklearn.cluster

try:
    import umap.umap_ as umap
except:
    import umap

filename = "sample.fcs"

n_clusters = 5
# Set as False if you display all points
sampling_mode = True 

# Decomposite and cluster results of each cell
filename_results = "results.tsv"
# Average color intensity of each cluster
filename_clusters = "clusters.tsv"
verbose = True

# Algorithm selection
algorithm = None
algorithm = 'PCA'
algorithm = "tSNE"
algorithm = 'UMAP'

clustering = None
clustering = 'KMeans'
clustering = 'Agglomerative'
clustering = "DBSCAN"

logarithm_mode = False
axes = 0, 1, 2
if sampling_mode:
    max_samples = 2500
else:
    max_samples = 0
    
n_components = max(axes) + 1 # dimension of decomposition

def generate_test_data(n_samples=1000, n_dimensions=4, n_groups=4):
    centers = [(0,0,0,0,0), (3,3,0,1,2), (-2,0,4,-1,-4), (1, -5, 3, 4,2)]
    n_dimensions = min(n_dimensions, len(centers[0]))
    n_groups = min(n_groups, len(centers))
    deviations = [2, .5, 1.2, .9][0:n_groups]
    data = []
    labels = []
    i = 0
    group = 0
    while group < n_groups and i < n_samples:
        stop = i + n_samples // n_groups
        labels += [group] * (stop - i)
        dv = deviations[group]
        center = centers[group]
        for j in range(i, stop):
            vals = [np.random.normal(center[k], dv) for k in range(n_dimensions)]
            data.append(vals)
            pass
        i = stop
        group += 1
        pass
    return np.array(data).T, labels

if filename is None:
    n_groups = 4
    n_dim = 5
    n_samples = 1000
    analysis_title = "{} random sample {} clusters, {} dimensions".format(n_samples, n_groups, n_dim)

    data, labels = generate_test_data(n_samples, n_dim, n_groups=n_clusters)
    sample_names = [str(l) for l in labels]
else:
    if verbose:
        sys.stderr.write("{} loading file\n".format(datetime.datetime.now().strftime("[%H:%M:%S]"), filename))
    analysis_title = pathlib.Path(filename).stem
    
    if filename.endswith(".fcs"):
        import fcsparser
        meta, data = fcsparser.parse(filename)
        data = data.T
        color2name = {}
        for key, val in meta.items():
            if isinstance(val, str) is False:
                continue
            m = re.match("\\$(\\w\\d+)N", key)
            if m:
                color_code = m.group(1)
                color_name_code = "$" + color_code + "S"
                if color_name_code in meta:
                    name = meta[color_name_code]
                    if name == "EQBeads": continue
                    color2name[val] = meta[color_name_code]
        applicable = [index for index in data.index if index in color2name]
        names = [color2name[n_] for n_ in applicable]
        data = data.loc[applicable]
        data.index = names
    else:
        if filename.endswith('.csv'):
            sep = ','
        else:
            sep = '\t'
        data = pd.read_csv(filename, sep=sep, index_col=0)
    if max_samples > 0 and max_samples < data.shape[1]:
        data = data[data.columns[0:max_samples]]
    labels = [0] * data.shape[0]
    sample_names = data.columns
    
n_samples = data.shape[1]

if logarithm_mode:
    minimum_value = 0.1
    data[data<minimum_value] = minimum_value
    data = np.log(data)

# Decomposition
if verbose and algorithm is not None:
    sys.stderr.write("{} decompose using {}\n".format(datetime.datetime.now().strftime("[%H:%M:%S]"), algorithm))

if algorithm == "tSNE":
    if n_components == 2:
        import MulticoreTSNE
        tsne = MulticoreTSNE.MulticoreTSNE(n_components=n_components, n_jobs=8)
        res = tsne.fit_transform(data.T)
    else:
        import sklearn.manifold
        tsne = sklearn.manifold.TSNE(n_components=n_components)
        res = tsne.fit_transform(data.T)
elif algorithm == "PCA":
    import sklearn.decomposition
    ipca = sklearn.decomposition.IncrementalPCA(n_components=n_components)
    res = ipca.fit_transform(data.T)
elif algorithm == 'UMAP':
    res = umap.UMAP(n_components=n_components).fit_transform(data.T)
else: # no decomposition, display raw data
    res = data.T
    pass

# Clustering
if verbose and clustering is not None:
    sys.stderr.write("{} cluster using {}\n".format(datetime.datetime.now().strftime("[%H:%M:%S]"), clustering))


if clustering == "DBSCAN":
    dbscan = sklearn.cluster.DBSCAN()
    dbscan.fit(res)#data.T)
    clustered = dbscan.labels_
elif clustering == 'KMeans':
    kmeans = sklearn.cluster.KMeans(n_clusters=n_clusters, n_jobs=4)
    kmeans.fit_predict(res)#data.T)
    clustered = kmeans.labels_
elif clustering == 'Agglomerative':
    aggl = sklearn.cluster.AgglomerativeClustering(n_clusters=n_clusters)
    aggl.fit_predict(res)#data.T)
    clustered = aggl.labels_
else: # no clustering
    clustered = labels
    pass

if filename_results is not None:
    with open(filename_results, "w") as fo:
        fo.write("ID")
        dec = "" if algorithm is None else algorithm
        for i in range(n_components):
            fo.write("\t{}{}".format(dec, i + 1))
        fo.write("\tCluster\n")
        for i, X in enumerate(res):
            fo.write(str(sample_names[i]))
            for x in X:
                fo.write("\t{}".format(x))
            fo.write("\t{}\n".format(clustered[i]))
if filename_clusters is not None:
    num_sets = max(clustered) + 1
    num_elements = data.shape[0]
    accum = np.zeros((num_sets, num_elements))
    num_items = np.zeros(num_sets, dtype=np.int)
    values = data.values
    for i, c in enumerate(clustered):
        if c >= 0:
            num_items[c] += 1
            for j in range(num_elements):
                accum[c][j] += values[j][i]
    with open(filename_clusters, "w") as fo:
        fo.write("Cluster\tNum")
        for index in data.index:
            fo.write("\t{}".format(index))
        fo.write("\n")
        for i in range(num_sets):
            n = num_items[i]
            if n == 0: continue
            fo.write("{}\t{}".format(i + 1, n))
            for v in accum[i]:
                fo.write("\t{:.1f}".format(v / n))
            fo.write("\n")
            
# Prepare diagram
if verbose and clustering is not None:
    sys.stderr.write("{} generating a diagram\n".format(datetime.datetime.now().strftime("[%H:%M:%S]")))

traces = []
given_labels = list(set(clustered))
n_clusters = len(given_labels)
for i in range(n_clusters):
    cluster = given_labels[i]
    indices = []
    for j in range(n_samples):
        if clustered[j] == cluster:
            indices.append(j)
    if len(indices) > 0:
        points = res[indices]
        if len(axes) == 2:
            traces.append(go.Scattergl(mode='markers', x=points[:,axes[0]], y=points[:,axes[1]]))
        elif len(axes) == 3:
            name = 'unclustered' if cluster < 0 else 'C {} ({})'.format(cluster + 1, points.shape[0])
            traces.append(go.Scatter3d(mode="markers", name=name, 
                                       x=points[:,axes[0]], y=points[:,axes[1]], z=points[:,axes[2]], marker=dict(size=2)))
        else:
            raise Exception('invalid number of components, 2 or 3 is allowed')
        pass
    pass

# Generate figure
if len(traces) == 0:
    raise Exception('no clusters detected')
else:
    # plotly.offline.init_notebook_mode(connected=True)

    titles = ["{}{}".format(algorithm if algorithm is not None else 'Raw', axis + 1) for axis in axes]
    if n_components == 3:
        layout = go.Layout(title=analysis_title, scene=dict(xaxis=dict(title=titles[0]), yaxis=dict(title=titles[1]), zaxis=dict(title=titles[2])))
    else:
        layout = go.Layout(title=analysis_title, xaxis=dict(title=titles[0]), yaxis=dict(title=titles[1]))
    fig = go.Figure(traces, layout)
    plotly.offline.iplot(fig)
    # print(fig)
    plotly.offline.plot(fig, filename='jupyter_out.html')
